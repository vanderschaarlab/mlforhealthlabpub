{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#performance metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import normalized_mutual_info_score, homogeneity_score, adjusted_rand_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "#user defined\n",
    "from utils_log import save_logging, load_logging\n",
    "from data_loader import import_data\n",
    "from class_AC_TPC import AC_TPC, initialize_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_minibatch(mb_size, x, y):\n",
    "    idx = range(np.shape(x)[0])\n",
    "    idx = random.sample(idx, mb_size)\n",
    "\n",
    "    x_mb   = x[idx].astype(float)    \n",
    "    y_mb   = y[idx].astype(float)    \n",
    "\n",
    "    return x_mb, y_mb\n",
    "\n",
    "### PERFORMANCE METRICS:\n",
    "def f_get_prediction_scores(y_true_, y_pred_):\n",
    "    if np.sum(y_true_) == 0: #no label for running roc_auc_curves\n",
    "        auroc_ = -1.\n",
    "        auprc_ = -1.\n",
    "    else:\n",
    "        auroc_ = roc_auc_score(y_true_, y_pred_)\n",
    "        auprc_ = average_precision_score(y_true_, y_pred_)\n",
    "    return (auroc_, auprc_)\n",
    "\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    c_matrix = contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(c_matrix, axis=0)) / np.sum(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'sample'\n",
    "data_x, data_y, y_type = import_data(data_name = data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "tr_data_x,te_data_x, tr_data_y,te_data_y = train_test_split(\n",
    "    data_x, data_y, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "tr_data_x,va_data_x, tr_data_y,va_data_y = train_test_split(\n",
    "    tr_data_x, tr_data_y, test_size=0.2, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE NETWORK PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "\n",
    "h_dim_FC   = 50 #for fully_connected layers\n",
    "h_dim_RNN  = 50\n",
    "\n",
    "x_dim = np.shape(data_x)[2]\n",
    "y_dim = np.shape(data_y)[2]\n",
    "  \n",
    "    \n",
    "num_layer_encoder    = 1\n",
    "num_layer_selector   = 2\n",
    "num_layer_predictor  = 2\n",
    "\n",
    "z_dim = h_dim_RNN * num_layer_encoder\n",
    "\n",
    "max_length = np.shape(data_x)[1]\n",
    "\n",
    "rnn_type          = 'LSTM' #GRU, LSTM\n",
    "\n",
    "\n",
    "input_dims ={\n",
    "    'x_dim': x_dim,\n",
    "    'y_dim': y_dim,\n",
    "    'y_type': y_type,\n",
    "    'max_cluster': K,\n",
    "    'max_length': max_length    \n",
    "}\n",
    "\n",
    "network_settings ={\n",
    "    'h_dim_encoder': h_dim_RNN,\n",
    "    'num_layers_encoder': num_layer_encoder,\n",
    "    'rnn_type': rnn_type,\n",
    "    'rnn_activate_fn': tf.nn.tanh,\n",
    "\n",
    "    'h_dim_selector': h_dim_FC,\n",
    "    'num_layers_selector': num_layer_selector,\n",
    "    \n",
    "    'h_dim_predictor': h_dim_FC,\n",
    "    'num_layers_predictor': num_layer_predictor,\n",
    "    \n",
    "    'fc_activate_fn': tf.nn.relu\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN -- INITIALIZE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate    = 0.001\n",
    "keep_prob  = 0.7\n",
    "mb_size    = 128\n",
    "\n",
    "ITERATION  = 10000\n",
    "check_step = 1000\n",
    "\n",
    "save_path = './{}/proposed/init/'.format(data_name)\n",
    "\n",
    "if not os.path.exists(save_path + '/models/'):\n",
    "    os.makedirs(save_path + '/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initialize Network...')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Turn on xla optimization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = AC_TPC(sess, \"AC_TPC\", input_dims, network_settings)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer(), feed_dict={model.E:np.zeros([K, z_dim]).astype(float)})\n",
    "\n",
    "avg_loss  = 0\n",
    "for itr in range(ITERATION):\n",
    "    x_mb, y_mb  = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
    "\n",
    "    _, tmp_loss = model.train_mle(x_mb, y_mb, lr_rate, keep_prob)\n",
    "    avg_loss   += tmp_loss/check_step\n",
    "\n",
    "    if (itr+1)%check_step == 0:                \n",
    "        tmp_y, tmp_m = model.predict_y_hats(va_data_x)\n",
    "\n",
    "        y_pred = tmp_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "        y_true = va_data_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "\n",
    "        AUROC = np.zeros([y_dim])\n",
    "        AUPRC = np.zeros([y_dim])\n",
    "        for y_idx in range(y_dim):\n",
    "            auroc, auprc = f_get_prediction_scores(y_true[:, y_idx], y_pred[:, y_idx])\n",
    "            AUROC[y_idx] = auroc\n",
    "            AUPRC[y_idx] = auprc\n",
    "\n",
    "        print (\"ITR {:05d}: loss_2={:.3f} | va_auroc:{:.3f}, va_auprc:{:.3f}\".format(\n",
    "                itr+1, avg_loss, np.mean(AUROC), np.mean(AUPRC))\n",
    "              )        \n",
    "        avg_loss = 0\n",
    "\n",
    "saver.save(sess, save_path + 'models/model_K{}'.format(K))\n",
    "save_logging(network_settings, save_path + 'models/network_settings_K{}.txt'.format(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN -- TEMPORAL PHENOTYPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha  = 1.0\n",
    "beta   = 0.01\n",
    "\n",
    "mb_size    = 128\n",
    "M          = int(tr_data_x.shape[0]/mb_size) #for main algorithm\n",
    "keep_prob  = 0.7\n",
    "lr_rate1   = 1e-3\n",
    "lr_rate2   = 1e-3\n",
    "\n",
    "save_path = './{}/proposed/trained/'.format(data_name)\n",
    "\n",
    "if not os.path.exists(save_path + '/models/'):\n",
    "    os.makedirs(save_path + '/models/')\n",
    "\n",
    "if not os.path.exists(save_path + '/results/'):\n",
    "    os.makedirs(save_path + '/results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD INITIALIZED NETWORK\n",
    "\n",
    "load_path = './{}/proposed/init/'.format(data_name)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Turn on xla optimization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "network_settings = load_logging(load_path + 'models/network_settings_K{}.txt'.format(K))\n",
    "z_dim = network_settings['num_layers_encoder'] * network_settings['h_dim_encoder']\n",
    "\n",
    "model = AC_TPC(sess, \"AC_TPC\", input_dims, network_settings)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, load_path + 'models/model_K{}'.format(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================')\n",
    "print('===== INITIALIZING EMBEDDING & SELECTOR =====')\n",
    "# K-means over the latent encodings\n",
    "e, s_init, tmp_z = initialize_embedding(model, tr_data_x, K)\n",
    "e = np.arctanh(e)\n",
    "sess.run(model.EE.initializer, feed_dict={model.E:e}) #model.EE = tf.nn.tanh(model.E)\n",
    "\n",
    "# update selector wrt initial classes\n",
    "ITERATION  = 5000\n",
    "check_step = 1000\n",
    "\n",
    "avg_loss_s = 0\n",
    "for itr in range(ITERATION):\n",
    "    z_mb, s_mb = f_get_minibatch(mb_size, tmp_z, s_init)\n",
    "    _, tmp_loss_s = model.train_selector(z_mb, s_mb, lr_rate1, k_prob=keep_prob)\n",
    "\n",
    "    avg_loss_s += tmp_loss_s/check_step\n",
    "    if (itr+1)%check_step == 0:\n",
    "        print(\"ITR:{:04d} | Loss_s:{:.4f}\".format(itr+1, avg_loss_s) )\n",
    "        avg_loss_s = 0\n",
    "\n",
    "tmp_ybars = model.predict_yy(np.tanh(e))\n",
    "new_e     = np.copy(e)\n",
    "print('=============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================================')\n",
    "print('========== TRAINING MAIN ALGORITHM ==========')\n",
    "'''\n",
    "    L1: predictive clustering loss\n",
    "    L2: sample-wise entropy loss\n",
    "    L3: embedding separation loss\n",
    "'''\n",
    "\n",
    "ITERATION     = 3000\n",
    "check_step    = 10\n",
    "\n",
    "avg_loss_c_L1 = 0\n",
    "avg_loss_a_L1 = 0\n",
    "avg_loss_a_L2 = 0\n",
    "avg_loss_e_L1 = 0 \n",
    "avg_loss_e_L3 = 0\n",
    "\n",
    "va_avg_loss_L1 = 0\n",
    "va_avg_loss_L2 = 0\n",
    "va_avg_loss_L3 = 0\n",
    "\n",
    "for itr in range(ITERATION):        \n",
    "    e = np.copy(new_e)\n",
    "\n",
    "    for _ in range(M):\n",
    "        x_mb, y_mb = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
    "\n",
    "        _, tmp_loss_c_L1  = model.train_critic(x_mb, y_mb, lr_rate1, keep_prob)\n",
    "        avg_loss_c_L1    += tmp_loss_c_L1/(M*check_step)\n",
    "\n",
    "        x_mb, y_mb = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
    "\n",
    "        _, tmp_loss_a_L1, tmp_loss_a_L2 = model.train_actor(x_mb, y_mb, alpha, lr_rate2, keep_prob)\n",
    "        avg_loss_a_L1 += tmp_loss_a_L1/(M*check_step)\n",
    "        avg_loss_a_L2 += tmp_loss_a_L2/(M*check_step)\n",
    "        \n",
    "    for _ in range(M):\n",
    "        x_mb, y_mb = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
    "\n",
    "        _, tmp_loss_e_L1, tmp_loss_e_L3 = model.train_embedding(x_mb, y_mb, beta, lr_rate1, keep_prob)\n",
    "        avg_loss_e_L1  += tmp_loss_e_L1/(M*check_step)\n",
    "        avg_loss_e_L3  += tmp_loss_e_L3/(M*check_step)\n",
    "\n",
    "        \n",
    "    x_mb, y_mb = f_get_minibatch(mb_size, va_data_x, va_data_y)\n",
    "    tmp_loss_L1, tmp_loss_L2, tmp_loss_L3 = model.get_losses(x_mb, y_mb)\n",
    "    \n",
    "    va_avg_loss_L1  += tmp_loss_L1/check_step\n",
    "    va_avg_loss_L2  += tmp_loss_L2/check_step\n",
    "    va_avg_loss_L3  += tmp_loss_L3/check_step\n",
    "\n",
    "    new_e = sess.run(model.embeddings)\n",
    "\n",
    "    if (itr+1)%check_step == 0:\n",
    "        tmp_ybars = model.predict_yy(new_e)\n",
    "        print (\"ITR {:04d}: L1_c={:.3f}  L1_a={:.3f}  L1_e={:.3f}  L2={:.3f}  L3={:.3f} || va_L1={:.3f}  va_L2={:.3f}  va_L3={:.3f}\".format(\n",
    "            itr+1, avg_loss_c_L1, avg_loss_a_L1, avg_loss_e_L1, avg_loss_a_L2, avg_loss_e_L3,\n",
    "            va_avg_loss_L1, va_avg_loss_L2, va_avg_loss_L3\n",
    "        ))\n",
    "        avg_loss_c_L1 = 0\n",
    "        avg_loss_a_L1 = 0\n",
    "        avg_loss_a_L2 = 0\n",
    "        avg_loss_e_L1 = 0\n",
    "        avg_loss_e_L3 = 0\n",
    "        va_avg_loss_L1 = 0\n",
    "        va_avg_loss_L2 = 0\n",
    "        va_avg_loss_L3 = 0\n",
    "print('=============================================')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_avg_loss_L1 + alpha*va_avg_loss_L2 + beta*va_avg_loss_L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(sess, save_path + 'models/model_K{}'.format(K))\n",
    "\n",
    "save_logging(network_settings, save_path + 'models/network_settings_K{}.txt'.format(K))\n",
    "np.savez(save_path + 'models/embeddings.npz', e=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, save_path + 'models/model_K{}'.format(K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tmp_pi, tmp_m = model.predict_zbars_and_pis_m2(te_data_x)\n",
    "\n",
    "tmp_pi = tmp_pi.reshape([-1, K])[tmp_m.reshape([-1]) == 1]\n",
    "\n",
    "ncol = nrow = int(np.ceil(np.sqrt(K)))\n",
    "plt.figure(figsize=[4*ncol, 2*nrow])\n",
    "for k in range(K):\n",
    "    plt.subplot(ncol, nrow, k+1)\n",
    "    plt.hist(tmp_pi[:, k])\n",
    "plt.suptitle(\"Clustering assignment probabilities\")\n",
    "plt.show()\n",
    "# plt.savefig(save_path + 'results/figure_clustering_assignments.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# In[147]:\n",
    "\n",
    "\n",
    "# check selector outputs and intialized classes\n",
    "pred_y, tmp_m = model.predict_s_sample(tr_data_x)\n",
    "\n",
    "pred_y = pred_y.reshape([-1, 1])[tmp_m.reshape([-1]) == 1]\n",
    "print(np.unique(pred_y))\n",
    "\n",
    "plt.hist(pred_y[:, 0], bins=15, color='C1', alpha=1.0)\n",
    "plt.show()\n",
    "# plt.savefig(save_path + 'results/figure_clustering_hist.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "tmp_y, tmp_m = model.predict_y_bars(te_data_x)\n",
    "\n",
    "\n",
    "y_pred = tmp_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "y_true = te_data_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "\n",
    "\n",
    "AUROC = np.zeros([y_dim])\n",
    "AUPRC = np.zeros([y_dim])\n",
    "for y_idx in range(y_dim):\n",
    "    auroc, auprc = f_get_prediction_scores(y_true[:, y_idx], y_pred[:, y_idx])\n",
    "    AUROC[y_idx] = auroc\n",
    "    AUPRC[y_idx] = auprc\n",
    "\n",
    "print('AUROC: {}'.format(AUROC))\n",
    "print('AUPRC: {}'.format(AUPRC))\n",
    "\n",
    "pred_y, tmp_m = model.predict_s_sample(te_data_x)\n",
    "\n",
    "pred_y = (pred_y * tmp_m).reshape([-1, 1])\n",
    "pred_y = pred_y[(tmp_m.reshape([-1, 1]) == 1)[:, 0], 0]\n",
    "\n",
    "true_y = (te_data_y * np.tile(np.expand_dims(tmp_m, axis=2), [1,1,y_dim])).reshape([-1, y_dim])\n",
    "true_y = true_y[(tmp_m.reshape([-1]) == 1)]\n",
    "true_y = np.argmax(true_y, axis=1)\n",
    "\n",
    "tmp_nmi    = normalized_mutual_info_score(true_y, pred_y)\n",
    "tmp_ri     = adjusted_rand_score(true_y, pred_y)\n",
    "tmp_purity = purity_score(true_y, pred_y)\n",
    "\n",
    "print('NMI:{:.4f}, RI:{:.4f}, PURITY:{:.4f}'.format(tmp_nmi, tmp_ri, tmp_purity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUPRC.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.layers import GlobalAveragePooling2D, BatchNormalization, Input, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import keras.optimizers\n",
    "import pydot\n",
    "import networkx as nx\n",
    "from IPython.display import SVG\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from pycausal import search as s\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "from pycausal import prior as p\n",
    "from causal_assurance import *\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Select your number of models\n",
    "num_models = 100\n",
    "\n",
    "# CAM-10 specified by nbest as percentage here.\n",
    "nbest = int(num_models * 0.1)\n",
    "\n",
    "# Select your number of iterations to repeat experiment\n",
    "num_repeat = 100\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" #Comment this line out if you want all GPUS \n",
    "\n",
    "dataset_path = '../data/hour.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"Temp\", \"Atemp\", \"Humidity\", \"Windspeed\"]\n",
    "target =  [\"Count\"]\n",
    "\n",
    "df = pd.read_csv(dataset_path, names = [\"Temp\", \"Atemp\", \"Humidity\", \"Windspeed\", \"Count\"], usecols = [10, 11, 12, 13, 16])\n",
    "df = df[1:]\n",
    "categoricals = [] \n",
    "\n",
    "temporal = [['Temp', 'Atemp', 'Humidity', 'Windspeed'], ['Count']]\n",
    "prior = p.knowledge(requiredirect= [('Temp', 'Atemp'),('Windspeed', 'Atemp')],\n",
    "                   addtemporal = temporal\n",
    "                   )\n",
    "g = examine_graph_continuous(df, prior, method = 1)\n",
    "dot_str = pc.tetradGraphToDot(g)\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)\n",
    "\n",
    "known_conx = set({})\n",
    "for i in tetrad.getEdges():\n",
    "    if ' o-> ' in i:\n",
    "        known_conx.add((i.split(' o-> ')[0], i.split(' o-> ')[1]))\n",
    "    elif ' --> ' in i:\n",
    "        known_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "\n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, known_conx)),)\n",
    "\n",
    "n_holdout = int(len(df) * 0.2)\n",
    "df['Count'] = normalize(df['Count'].values.astype(int))\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "df['Temp'] = df['Temp'].values.astype(float)\n",
    "df['Atemp'] = df['Atemp'].values.astype(float)\n",
    "df['Windspeed'] = df['Windspeed'].values.astype(float)\n",
    "df['Humidity'] = df['Humidity'].values.astype(float)\n",
    "original_df = df.copy()\n",
    "randomize = False\n",
    "if randomize:\n",
    "    layers = [256, 512, 1024, 2048, 4096]\n",
    "    for i in range(num_models):\n",
    "        network = []\n",
    "        for j in range(3):\n",
    "            network.append(layers[random.randint(0,len(layers) -1)])\n",
    "        models.append(network)\n",
    "        model_names.append('models/random' + str(i))\n",
    "    print(models, model_names)    \n",
    "else:\n",
    "    model_layers = [512,256]\n",
    "    for i in range(num_models):\n",
    "        models.append(model_layers)\n",
    "        model_names.append('models/bike' + str(i))\n",
    "        \n",
    "# found connections, and let's orient windspeed to humidity\n",
    "prior = p.knowledge(requiredirect= [('Temp', 'Atemp'),('Windspeed', 'Atemp'),('Windspeed', 'Humidity'), ('Temp', 'Count'),('Humidity', 'Count')])\n",
    "prior\n",
    "print(models, model_names)\n",
    "SVG(svg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestMSE = []\n",
    "bestCOMBO = []\n",
    "for t in range(num_repeat):\n",
    "    # let's split our df into two by race.  Let's see what happens if we \n",
    "    df = original_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "    holdout = int(len(df) * 0.2)\n",
    "        #df_test = df[df['charges'] > 0.54].copy()\n",
    "    continuous = [\"Count\",  \"Humidity\", \"Atemp\", \"Windspeed\"]\n",
    "    \n",
    "    \n",
    "    small = random.randint(0,1)\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    if small == 0:\n",
    "        df_test = df.nsmallest(holdout, continuous[cont])\n",
    "    else:\n",
    "        df_test = df.nlargest(holdout, continuous[cont])\n",
    "    \n",
    "    print(t, small, continuous[cont])\n",
    "\n",
    "    '''\n",
    "        end_idx = len(df) - holdout\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    start_idx = random.randint(0, end_idx)\n",
    "    print(t, \"Doing range:\",start_idx, start_idx + holdout, \"and \", continuous[cont])\n",
    "    df_test = df.nlargest(len(df) - start_idx, continuous[cont]).nsmallest(holdout, continuous[cont])\n",
    "    '''\n",
    "\n",
    "\n",
    "    \n",
    "    df.drop(df_test.index, inplace = True)\n",
    "    df_test.reset_index(inplace = True)\n",
    "    df.sample(frac= 1).reset_index(inplace = True) # this will shuffle and reset index\n",
    "\n",
    "    x_test = df_test[inputs]\n",
    "    y_test = df_test[target]\n",
    "\n",
    "    causal_split = 0.2\n",
    "    val_split = 0.2\n",
    "    train_split = 1 - (causal_split + val_split)\n",
    "\n",
    "    x_causal = df[inputs][-int(causal_split * len(df)) :]\n",
    "    y_causal = df[target][-int(causal_split * len(df)) :]\n",
    "\n",
    "    x_val = df[inputs][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "    y_val = df[target][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "\n",
    "    x_train = df[inputs][:int(train_split * len(df))]\n",
    "    y_train = df[target][:int(train_split * len(df))]\n",
    "    len(x_causal), len(y_causal), len(x_val), len(y_val), len(x_train), len(y_train)\n",
    "\n",
    "\n",
    "\n",
    "    x_test_NN = x_test\n",
    "    x_causal_NN = x_causal\n",
    "    x_val_NN = x_val\n",
    "    x_train_NN = x_train\n",
    "\n",
    "    verbosity = 0\n",
    "\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "\n",
    "        if type(models[idx]) is list:\n",
    "            #clear session\n",
    "            keras.backend.clear_session() \n",
    "            #get model according to specification\n",
    "            model = get_model(models[idx], [0.2] * len(models), np.shape(x_train_NN)[1])\n",
    "            callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                         EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "            model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "            model.fit(x_train_NN, y_train, epochs = 20, validation_data = (x_val_NN, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "        else:\n",
    "            models[idx].fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    x_causal.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(x_test_NN)\n",
    "        generalization.append(mean_squared_error(y_pred, y_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal_NN)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "        causal_targets.reset_index(drop=True, inplace = True)\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "        metrics.append(mean_squared_error(y_causal_pred, y_causal))\n",
    "\n",
    "        ll_pred = get_ll_continuous(causal_df, prior)\n",
    "\n",
    "        proposed.append(ll_pred)\n",
    "\n",
    "    total = normalize(metrics) + normalize(proposed)\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed, total, normalize(generalization)), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "    print(\"MSE = \", np.mean(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "    print(\"COMB = \",np.mean(final.nsmallest(nbest, 'combined')['generalization']))\n",
    "    bestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'].values)\n",
    "    bestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'].values)\n",
    "    \n",
    "\n",
    "np.mean(bestMSE), np.mean(bestCOMBO), np.std(bestMSE), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "val1 = []\n",
    "for each in bestMSE:\n",
    "    val1.append(np.mean(each))\n",
    "val2 = []\n",
    "for each in bestCOMBO:\n",
    "    val2.append(np.mean(each))\n",
    "\n",
    "val = []\n",
    "for x, y in zip(val1, val2):\n",
    "    val.append([x, y])\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(2.5,3)\n",
    "df = pd.DataFrame(val, columns = ['MSE', 'Proposed'])\n",
    "ax = sns.boxplot(ax = ax, data=df, palette=\"Set2\")\n",
    "fig.savefig('kaggle-bikeshare.pdf')\n",
    "d = dict()\n",
    "d['bestMSE'] = bestMSE\n",
    "d['bestCOMBO'] = bestCOMBO\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('BikeShare.pkl', 'wb') as handle:\n",
    "    pickle.dump(d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

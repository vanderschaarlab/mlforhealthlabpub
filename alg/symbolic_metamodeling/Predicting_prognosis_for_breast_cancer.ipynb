{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting prognosis for breast cancer\n",
    "\n",
    "This notebook implements the experiments in Section 5.2 in our paper *\"Demystifying Black-box Models with Symbolic Metamodels\"* submitted to **NeurIPS 2019** by *Ahmed M. Alaa and Mihaela van der Schaar*. The experiments are based on the PREDICT dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we demonstrate the utility of symbolic metamodeling in a real-world setup for which model interpretability and transparency are of immense importance. In particular, we consider the problem of predicting the risk of mortality for breast cancer patients based on clinical features. For this setup, the ACCJ guidelines require prognostic models to be formulated as transparent equations [1] — symbolic metamodeling can enable machine learning models to meet these requirements by converting black-box prognostic models into risk equations that can be written on a piece of paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting our experiments, we first import the required libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysymbolic.utilities.performance import *\n",
    "from pysymbolic.algorithms.symbolic_metamodeling import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from data.PREDICT_score import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data for 2,000 breast cancer patients extracted from the UK cancer registry, we fit an XGBoost model $f(x)$ to predict the patients' 5 year mortality risk based on 7 features: age, screening status, number of nodes, tumor size, tumor grade, ER and HER2 status. We first read the PREDICT dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_data     = pd.read_csv('data/PREDICT_data_subset.csv').drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "predict_features = ['AGE', 'ScreeningvsClinical', 'TUMOURSIZE','GRADEG1', 'GRADEG2', 'GRADEG3', 'GRADEG4', \n",
    "                    'NODESINVOLVED', 'ER_STATUSN', 'HER2_STATUSP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5-fold cross-validation, we compare the area under receiver operating characteristic (AUC-ROC) accuracy of the XGBoost model with that of the PREDICT risk calculator [https://breast.predict.nhs.uk/](https://breast.predict.nhs.uk/), which is the risk equation most commonly used in current practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model_mode, PREDICT_data, num_folds):\n",
    "\n",
    "    PREDICT_pref  = []\n",
    "    num_samples   = len(PREDICT_data)\n",
    "    \n",
    "    train_size    = int(np.floor(num_samples * (1 - 1/num_folds)))\n",
    "    test_size     = num_samples - train_size\n",
    "    \n",
    "    test_indexes  = partition_(scrambled(list(range(num_samples))), num_folds)\n",
    "\n",
    "    minmaxscaler  = MinMaxScaler()\n",
    "    \n",
    "    minmaxscaler.fit(PREDICT_data[predict_features])\n",
    "\n",
    "    for u in range(num_folds):\n",
    "    \n",
    "        train_indexes = list(set(range(num_samples)) - set(test_indexes[u]))\n",
    "    \n",
    "        train_data    = PREDICT_data.loc[PREDICT_data.index[train_indexes]]\n",
    "        test_data     = PREDICT_data.loc[PREDICT_data.index[test_indexes[u]]]\n",
    "        \n",
    "        if model_mode == 'XGBoost':\n",
    "            \n",
    "            model = XGBClassifier(n_estimators=300) \n",
    "            \n",
    "        elif model_mode == 'PREDICT':    \n",
    "    \n",
    "            model = PREDICT_model(minmaxscaler) \n",
    "    \n",
    "    \n",
    "        x_train       = train_data[predict_features]\n",
    "        x_test        = test_data[predict_features]\n",
    "        y_train       = train_data['Label'].astype(int)\n",
    "        y_test        = test_data['Label'].astype(int)\n",
    "    \n",
    "        x_train   = minmaxscaler.transform(x_train)\n",
    "        x_test    = minmaxscaler.transform(x_test)\n",
    "        \n",
    "        if model_mode == 'XGBoost':\n",
    "            \n",
    "            model.fit(x_train, y_train)\n",
    "        \n",
    "        y_pred        = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "        PREDICT_pref.append(roc_auc_score(y_test, y_pred)) \n",
    "        \n",
    "    return mean_confidence_interval(PREDICT_pref)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start by validating the XGBoost model on the PREDICT dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(\"XGBoost\", PREDICT_data, num_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number above corresponds to the average AUCROC of the model in addition to the 95% confidence interval. Now let us examine the performance of the PREDICT risk score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(\"PREDICT\", PREDICT_data, num_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the XGBoost model significantly outperforms the PREDICT model. In the next Section, we show how metamodeling can help us understand the sources of the performance gains achieved by XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic metamodeling for the XGBoost and PREDICT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by obtaining the symbolic metamodel for the XGBoost model. We do so by first fitting the XGBoost model to the entire dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBmodel      = XGBClassifier(n_estimators=300)  \n",
    "x             = PREDICT_data[predict_features]\n",
    "minmaxscaler  = MinMaxScaler()\n",
    "\n",
    "x             = minmaxscaler.fit_transform(x)\n",
    "y             = PREDICT_data['Label']    \n",
    "\n",
    "XGBmodel.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we use sklearn's MinMaxScaler to put all features in the range $[0,1]$. Now to obtain the metamodel for XGBoost, we first create an instance of the **Symbolic_Metamodel** class (which is in the **pysymbolic.algorithms.metamodeling** with default settings as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_metamodel = symbolic_metamodel(XGBmodel, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of the metamodel can be summarized as follows: \n",
    "\n",
    "- **n_dim**: the number of features in the model.\n",
    "- **batch_size**, **num_iter** and **learning_rate**: the parameters of the gradient descent learning algorithm.\n",
    "- **feature_types**: a list of data types for each feature. \"b\" corresponds to binary features whereas \"c\" corresponds to continuous ones. \n",
    "\n",
    "Now let us fit the metamodel simply by calling the **fit** method after supplying the trained model and the training features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_metamodel.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are done fitting the metamodel! Now to see the symbolic expression it learned, we need to inspect the **metamodel** attribute in the **Symbolic_Metamodel** object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_printing()\n",
    "\n",
    "XGBoost_metamodel.approx_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check the AUCROC accuracy of the metamodel. This can be done by evaluating the metamodel's prediction using the **evaluate** method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_metamodel = XGBoost_metamodel.evaluate(x)\n",
    "roc_auc_score(y, y_metamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let us repeat the same procedure for the PREDICT score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x              = PREDICT_data[predict_features]\n",
    "minmaxscaler   = MinMaxScaler()\n",
    "\n",
    "x              = minmaxscaler.fit_transform(x)\n",
    "PREDICT_model_ = PREDICT_model(minmaxscaler)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_metamodel = symbolic_metamodel(PREDICT_model_, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_metamodel.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_metamodel.approx_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the PREDICT metamodel perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_metamodel = PREDICT_metamodel.evaluate(x)\n",
    "roc_auc_score(y, y_predict_metamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to explain the differences between the XGBoost and PREDICT models, let us compare the average instancewise feature importance for every feature as assigned by the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_instancewise_scores      = XGBoost_metamodel.get_instancewise_scores(x)\n",
    "XGB_instancewise_scores_mean = np.mean(np.array(XGB_instancewise_scores).reshape((-1, x.shape[1])), axis=0)\n",
    "XGB_instancewise_scores_mean = XGB_instancewise_scores_mean /np.sum(XGB_instancewise_scores_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_instancewise_scores       = PREDICT_metamodel.get_instancewise_scores(x)\n",
    "PREDICT_instancewise_scores_mean  = np.mean(np.array(PREDICT_instancewise_scores).reshape((-1, x.shape[1])), axis=0)\n",
    "PREDICT_instancewise_scores_mean  = PREDICT_instancewise_scores_mean/np.sum(PREDICT_instancewise_scores_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2.5))\n",
    "\n",
    "ax1=plt.subplot(1,1,1)\n",
    "\n",
    "ind   = np.arange(len(PREDICT_instancewise_scores_mean))  \n",
    "width = 0.15                                  \n",
    "\n",
    "rects1  = ax1.bar(ind, PREDICT_instancewise_scores_mean, width, label='PREDICT: AUC-ROC = 0.762 +/- 0.02', color='b')\n",
    "rects2  = ax1.bar(ind + width, XGB_instancewise_scores_mean, width, label='XGBoost: AUC-ROC = 0.833 +/- 0.02', color='r')\n",
    "\n",
    "ax1.set_ylabel('Feature importance')\n",
    "ax1.set_xticks(ind)\n",
    "ax1.set_xticklabels(tuple(predict_features), rotation=40)\n",
    "ax1.legend(loc='upper_left', fontsize=12, frameon=True, fancybox=True)\n",
    "\n",
    "fig.savefig('feature_importances.pdf', dpi=200,  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the differences between the metamodels with respect to the median ranks of individual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysymbolic.utilities.instancewise_metrics import *\n",
    "\n",
    "XGB_ranks     = create_rank(np.array(XGB_instancewise_scores).reshape((-1, 10)), k=10)\n",
    "PREDICT_ranks = create_rank(np.array(PREDICT_instancewise_scores).reshape((-1, 10)), k=10) \n",
    "\n",
    "XGB_median_ranks     = [np.median(XGB_ranks[:, k]) for k in range(XGB_ranks.shape[1])]\n",
    "PREDICT_median_ranks = [np.median(PREDICT_ranks[:, k]) for k in range(PREDICT_ranks.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2.5))\n",
    "\n",
    "ax1=plt.subplot(1,1,1)\n",
    "\n",
    "ind   = np.arange(len(XGB_median_ranks))  \n",
    "width = 0.15                                  \n",
    "\n",
    "rects1  = ax1.bar(ind, PREDICT_median_ranks, width, label='PREDICT: AUC-ROC = 0.762 +/- 0.02', color='b')\n",
    "rects2  = ax1.bar(ind + width, XGB_median_ranks, width, label='XGBoost: AUC-ROC = 0.833 +/- 0.02', color='r')\n",
    "\n",
    "ax1.set_ylabel('Median Feature Rank')\n",
    "ax1.set_xticks(ind)\n",
    "ax1.set_xticklabels(tuple(predict_features), rotation=40)\n",
    "ax1.legend(loc='upper_left', fontsize=12, frameon=True, fancybox=True)\n",
    "ax1.set_ylim([0, 20])\n",
    "\n",
    "fig.savefig('feature_importances.pdf', dpi=200,  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Michael W Kattan, Kenneth R Hess, Mahul B Amin, Ying Lu, Karl GM Moons, Jeffrey E Gershenwald, Phyllis A Gimotty, Justin H Guinney, Susan Halabi, Alexander J Lazar, et al. American joint committee on cancer acceptance criteria for inclusion of risk models for individ329 ualized prognosis in the practice of precision medicine. CA: a cancer journal for clinicians, 66(5):370–374, 2016.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

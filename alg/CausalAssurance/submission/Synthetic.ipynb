{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.layers import GlobalAveragePooling2D, BatchNormalization, Input, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import keras.optimizers\n",
    "import pydot\n",
    "import networkx as nx\n",
    "from IPython.display import SVG\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from pycausal import search as s\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "from pycausal import prior as p\n",
    "\n",
    "\n",
    "#this is a file for shared functions\n",
    "from causal_assurance import * \n",
    "\n",
    "# Select your number of models\n",
    "num_models = 100\n",
    "\n",
    "# CAM-10 specified by nbest as percentage here.\n",
    "nbest = int(num_models * 0.1)\n",
    "\n",
    "# Select your number of iterations to repeat experiment\n",
    "num_repeat = 100\n",
    "\n",
    "# Select your graph size \n",
    "nodes = 4\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "\n",
    "#pc = pc()\n",
    "#pc.start_vm(java_max_heap_size = '21000M')\n",
    "#tetrad = s.tetradrunner()\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "\n",
    "# If you want to randomize networks set this to True\n",
    "randomize = False\n",
    "if randomize:\n",
    "    layers = [256, 512, 1024, 2048, 4096]\n",
    "    for i in range(num_models):\n",
    "        network = []\n",
    "        for j in range(3):\n",
    "            network.append(layers[random.randint(0,len(layers) -1)])\n",
    "        models.append(network)\n",
    "        model_names.append('model/sim' + str(i))\n",
    "\n",
    "else:\n",
    "    model_layers = [512, 256]\n",
    "    for i in range(num_models):\n",
    "        models.append(model_layers)\n",
    "        model_names.append('models/sim' + str(i))\n",
    "\n",
    "print(models, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the main function that will run for the specified number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for single variables.\n",
    "bestMSE = []\n",
    "bestCOMBO = []\n",
    "bestWRONG = []\n",
    "worstMSE = []\n",
    "worstCOMBO = []\n",
    "worstWRONG = []\n",
    "\n",
    "# for multiple variables.\n",
    "m_bestMSE = []\n",
    "m_bestCOMBO = []\n",
    "m_bestWRONG = []\n",
    "m_worstMSE = []\n",
    "m_worstCOMBO = []\n",
    "m_worstWRONG = []\n",
    "\n",
    "averageDegree = []\n",
    "targetDegree = []\n",
    "target_inD = []\n",
    "target_outD = []\n",
    "descendants = []\n",
    "graphDiff = []\n",
    "\n",
    "t = 0\n",
    "while (t < num_repeat):\n",
    "    train_size = 10000\n",
    "    test_mean = 1\n",
    "    test_var = 2\n",
    "    test_size = 2000\n",
    "    \n",
    "    # w_G is the imposter DAG\n",
    "    w_G = random_dag(nodes, random.randint(0, nodes * nodes)) # since max number of edges is n^2\n",
    "    require = []\n",
    "    for i in w_G.edges:\n",
    "        require.append([str(i[0]), str(i[1])])  \n",
    "    w_prior = p.knowledge(requiredirect = require)\n",
    "\n",
    "    \n",
    "    G = random_dag(nodes, random.randint(nodes, nodes * nodes)) # since max number of edges is n^2\n",
    "    df = gen_data2(np.arange(nodes), G.edges, SIZE = train_size)\n",
    "    require = []\n",
    "    for i in G.edges:\n",
    "        require.append([str(i[0]), str(i[1])])  \n",
    "    prior = p.knowledge(requiredirect = require)\n",
    "    examine_graph_continuous(df, prior)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Check to make sure that graph matches our prior knowledge. Or else abort this test.\n",
    "    a = set()\n",
    "    for i in tetrad.getEdges():\n",
    "        a.add((i[0], i[-1]))\n",
    "    b = set()\n",
    "    for i in require:\n",
    "        b.add((i[0], i[1]))\n",
    "    print(\"A = \", a)\n",
    "    print(\"B = \", b)\n",
    "    if a != b:\n",
    "        continue\n",
    "\n",
    "    # Need to set our inputs and outputs\n",
    "    inputs = set(np.arange(nodes))\n",
    "    target = str([x for x in G.nodes() if G.out_degree(x)==0 and G.in_degree(x)>=1][0])\n",
    "    inputs.remove(int(target))\n",
    "    inputs = list(map(str, inputs))\n",
    "    \n",
    "    perturb = int(inputs[random.randint(0,nodes - 2)])\n",
    "    df_test = gen_data2(np.arange(nodes), G.edges, mean = test_mean, var = test_var, SIZE = test_size)\n",
    "    sdf_test = gen_data1(np.arange(nodes), G.edges, mean = test_mean, var = test_var, SIZE = test_size, perturb = [perturb])\n",
    "    target = [target]\n",
    "    \n",
    "    print(\"Inputs = \", inputs)\n",
    "    print(\"Target = \", target)\n",
    "    \n",
    "    x_test = df_test[inputs]\n",
    "    y_test = df_test[target]\n",
    "    \n",
    "    sx_test = sdf_test[inputs]\n",
    "    sy_test = sdf_test[target]\n",
    "\n",
    "    causal_split = 0.2\n",
    "    val_split = 0.2\n",
    "    train_split = 1 - (causal_split + val_split)\n",
    "\n",
    "    x_causal = df[inputs][-int(causal_split * len(df)) :]\n",
    "    y_causal = df[target][-int(causal_split * len(df)) :]\n",
    "\n",
    "    x_val = df[inputs][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "    y_val = df[target][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "\n",
    "    x_train = df[inputs][:int(train_split * len(df))]\n",
    "    y_train = df[target][:int(train_split * len(df))]\n",
    "\n",
    "    verbosity = 0\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if idx % 10 == 0:\n",
    "            print(idx)\n",
    "        if type(models[idx]) is list:\n",
    "            #clear session\n",
    "            keras.backend.clear_session() \n",
    "            #get model according to specification\n",
    "            model = get_model(models[idx], [0.4] * len(models), np.shape(x_train)[1])\n",
    "            callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                         EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "            model.compile(optimizer = optimizers.SGD(lr = 0.0001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "            model.fit(x_train, y_train, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "        else:\n",
    "            models[idx].fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    w_proposed = []\n",
    "    x_causal.reset_index(drop=True, inplace = True)\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(x_test)\n",
    "        generalization.append(mean_squared_error(y_pred, y_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "        metrics.append(mean_squared_error(y_causal_pred, y_causal))\n",
    "\n",
    "        ll_pred = get_ll_continuous(causal_df, prior)\n",
    "        proposed.append(ll_pred)\n",
    "        ll_pred = get_ll_continuous(causal_df, w_prior)\n",
    "        w_proposed.append(ll_pred)\n",
    "\n",
    "\n",
    "    good_factor = (1 + len(nx.ancestors(G, int(target[0])))) / len(G.nodes)\n",
    "    wrong_factor = (1 + len(nx.ancestors(w_G, int(target[0])))) / len(w_G.nodes)\n",
    "    total = normalize(metrics) + normalize(proposed) * good_factor\n",
    "    wrong = normalize(metrics) + normalize(w_proposed) * wrong_factor\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed,  total, wrong, normalize(generalization)), axis = 1), columns = ['metrics', 'proposed', 'combined', 'wrong', 'generalization'])\n",
    "\n",
    "    bestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'].values)\n",
    "    bestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'].values)\n",
    "    bestWRONG.append(final.nsmallest(nbest, 'wrong')['generalization'].values)\n",
    "\n",
    "    worstMSE.append(final.nlargest(nbest, 'metrics')['generalization'].values)\n",
    "    worstCOMBO.append(final.nlargest(nbest, 'combined')['generalization'].values)\n",
    "    worstWRONG.append(final.nlargest(nbest, 'wrong')['generalization'].values)\n",
    "    \n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    w_proposed = []\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(sx_test)\n",
    "        generalization.append(mean_squared_error(y_pred, sy_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "        \n",
    "\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "\n",
    "        metrics.append(mean_squared_error(y_causal_pred, y_causal))\n",
    "\n",
    "        ll_pred = get_ll_continuous(causal_df, prior)\n",
    "        proposed.append(ll_pred)\n",
    "        ll_pred = get_ll_continuous(causal_df, w_prior)\n",
    "        w_proposed.append(ll_pred)\n",
    "\n",
    "\n",
    "    total = normalize(metrics) + normalize(proposed) * good_factor\n",
    "    wrong = normalize(metrics) + normalize(w_proposed) * wrong_factor\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed,  total, wrong, normalize(generalization)), axis = 1), columns = ['metrics', 'proposed', 'combined', 'wrong', 'generalization'])\n",
    "    m_bestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'].values)\n",
    "    m_bestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'].values)\n",
    "    m_bestWRONG.append(final.nsmallest(nbest, 'wrong')['generalization'].values)\n",
    "\n",
    "    m_worstMSE.append(final.nlargest(nbest, 'metrics')['generalization'].values)\n",
    "    m_worstCOMBO.append(final.nlargest(nbest, 'combined')['generalization'].values)\n",
    "    m_worstWRONG.append(final.nlargest(nbest, 'wrong')['generalization'].values) \n",
    "    \n",
    "    print(\"Times = \", t)\n",
    "    d = []\n",
    "    for i in G.degree():\n",
    "        d.append(i[1])\n",
    "        if str(i[0]) in target:\n",
    "            targetDegree.append(i[1])\n",
    "    averageDegree.append(np.mean(d))\n",
    "    target_inD.append(G.in_degree(int(target[0])))\n",
    "    target_outD.append(G.out_degree(int(target[0])))\n",
    "    descendants.append(len(nx.descendants(G, perturb)))\n",
    "    \n",
    "    graphDiff.append(nx.difference(G, w_G).edges())\n",
    "\n",
    "    \n",
    "    t += 1\n",
    "    \n",
    "np.mean(bestMSE), np.mean(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "d['bestMSE'] = bestMSE\n",
    "d['bestCOMBO'] = bestCOMBO\n",
    "d['bestWRONG'] = bestWRONG \n",
    "d['worstMSE'] = worstMSE\n",
    "d['worstCOMBO'] = worstCOMBO\n",
    "d['worstWRONG'] = worstWRONG\n",
    "\n",
    "d['m_bestMSE'] = m_bestMSE\n",
    "d['m_bestCOMBO'] = m_bestCOMBO\n",
    "d['m_bestWRONG'] = m_bestWRONG \n",
    "d['m_worstMSE'] = m_worstMSE\n",
    "d['m_worstCOMBO'] = m_worstCOMBO\n",
    "d['m_worstWRONG'] = m_worstWRONG\n",
    "\n",
    "d['averageDegree']= averageDegree\n",
    "d['targetDegree'] = targetDegree\n",
    "d['target_inD']=  target_inD\n",
    "d['target_outD'] = target_outD\n",
    "d['descendants'] = descendants\n",
    "d['graphDiff']= graphDiff\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(str(nodes) + 'node.pkl', 'wb') as handle:\n",
    "    pickle.dump(d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
